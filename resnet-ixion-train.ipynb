{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!wget -c http://images.cocodataset.org/zips/test2017.zip\n",
    "!echo unziping\n",
    "!unzip test2017.zip -d coco > /dev/null\n",
    "!rm test2017.zip"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e4e101d940e353bb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!wget -c http://images.cocodataset.org/zips/val2017.zip\n",
    "!echo unziping\n",
    "!unzip val2017.zip -d coco > /dev/null\n",
    "!rm val2017.zip"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "825dc5f361079fe9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!wget https://download.pytorch.org/models/resnet152-f82ba261.pth"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6f89bb1d3f9e79b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import models, transforms\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Hyperparameters and settings\n",
    "num_epochs = 4\n",
    "batch_size = 26\n",
    "learning_rate = 0.001"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "51ae0fb3a309e975"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_folder = ['coco/train2017', 'coco/test2017']\n",
    "val_folder = ['coco/val2017']\n",
    "allowed_extensions = ('.png', '.jpg', '.jpeg', '.bmp', '.webp')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a24956c1de86458"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# DO NOT CHANGE, unless you know what you are doing\n",
    "checkpoint_path = \"resnet152-f82ba261.pth\"\n",
    "classes = [0, 90, 180, 270]  # Four rotation classes: 0°, 90°, 180°, 270°\n",
    "num_workers = os.cpu_count()\n",
    "num_classes = len(classes)\n",
    "epoches_done = 0"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cfd66ef278f2dcf9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((336, 336)),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    # transforms.Resize((224, 224)),\n",
    "\n",
    "    transforms.RandomAutocontrast(),  # F.autocontrast,\n",
    "    # transforms.RandomEqualize(),\n",
    "    # v2.JPEG((50, 100)),\n",
    "\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cae9c8cbcfbced1f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def recursive_iterdir(path: Path):\n",
    "    path = Path(path)\n",
    "    for i in path.iterdir():\n",
    "        if i.is_dir():\n",
    "            yield from recursive_iterdir(i)\n",
    "        yield i"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1feafcea4ae5b6c4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class RotatedDataset(Dataset):\n",
    "    def __init__(self, folders: list[str], transform=None, limit: int = 0, offset: int = 0):\n",
    "        self.folders = [folders] if isinstance(folders, str) else folders\n",
    "        self.transform = transform\n",
    "        self.limit = limit\n",
    "        self.offset = offset\n",
    "        self.image_paths = []\n",
    "\n",
    "        # recursive iterate through directories\n",
    "        for f in self.folders:\n",
    "            for i in recursive_iterdir(f):\n",
    "                # image.PnG -> .PnG -> .png\n",
    "                if not i.suffix.lower().endswith(allowed_extensions):\n",
    "                    continue\n",
    "                self.image_paths.append(str(i))\n",
    "\n",
    "    def __len__(self):\n",
    "        total = len(self.image_paths) * num_classes\n",
    "        if self.limit:\n",
    "            total = self.limit\n",
    "        return total - self.offset\n",
    "\n",
    "    # the lst[0] returns the original image\n",
    "    # but the lst[1], lst[2], lst[3] returns rotations of the original image\n",
    "    # so in this way, each image would be x4'rd\n",
    "    def __getitem__(self, idx) -> tuple[torch.Tensor, int]:\n",
    "        idx = idx + self.offset\n",
    "        if idx >= len(self.image_paths) * num_classes:\n",
    "            raise IndexError\n",
    "\n",
    "        # the index of the original image in self.image_paths\n",
    "        index = idx // num_classes\n",
    "\n",
    "        # the index of the class.\n",
    "        # 0 = 0°, 1 = 90°, 2 = 180°, 3 = 270°\n",
    "        label = idx % num_classes\n",
    "\n",
    "        img_path = self.image_paths[index]\n",
    "        # noinspection PyBroadException\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        # some time some images are damaged, or not readable\n",
    "        # just return None, and they will be filtered by the collate_fn function\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "        # randomly horizontaly flip the image, like transforms.RandomHorizontalFlip, p = 0.5\n",
    "        if torch.rand(1).item() > 0.5:\n",
    "            image = image.transpose(Image.Transpose.FLIP_LEFT_RIGHT)\n",
    "\n",
    "        method = [\n",
    "            None,\n",
    "            Image.Transpose.ROTATE_90,\n",
    "            Image.Transpose.ROTATE_180,\n",
    "            Image.Transpose.ROTATE_270,\n",
    "        ][label]\n",
    "        if method is not None:\n",
    "            image = image.transpose(method)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b2fa6e1a55cd7d56"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_dataset = RotatedDataset(train_folder, transform=transform_train)\n",
    "val_dataset = RotatedDataset(val_folder, transform=transform_val)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c77746a1829abc1c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# filter bad / damaged images\n",
    "def collate_fn(batch):\n",
    "    batch = list(filter(lambda x: x is not None, batch))\n",
    "    return torch.utils.data.dataloader.default_collate(batch)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ac2f83fc186cf8e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, collate_fn=collate_fn)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a53ae6c73963e6dc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2f55ae8300347bc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = models.resnet152(weights=None)\n",
    "# load pretrained weights (skipping the final fc layer)\n",
    "state_dict = torch.load(checkpoint_path)\n",
    "model_dict = model.state_dict()\n",
    "pretrained_dict = {\n",
    "    k: v\n",
    "    for k, v in state_dict.items()\n",
    "    if k in model_dict and 'fc' not in k\n",
    "}\n",
    "model_dict.update(pretrained_dict)\n",
    "model.load_state_dict(model_dict)\n",
    "\n",
    "# get the number of input features for the existing fc layer (model.fc.in_features)\n",
    "# replace the final fully connected layer with a new one that outputs 4 classes\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "model = model.to(device)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f134e5b904ae1c4c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "53432a1dc600273"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for epoch in range(epoches_done + 1, num_epochs + 1):\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}] Starting...\")\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (images, labels) in enumerate(tqdm(train_loader)):\n",
    "        images: torch.Tensor = images.to(device)\n",
    "        labels: torch.Tensor = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(dim=0)\n",
    "\n",
    "    torch.save(model.state_dict(), f\"resnet152_ixion_e{epoch}.pth\")\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    # cleanup VRAM\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # a full validation of the val_dataset\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader):\n",
    "            images: torch.Tensor = images.to(device)\n",
    "            labels: torch.Tensor = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(dim=0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_acc = correct / total\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Validation Accuracy: {val_acc * 100:.4f}%\")\n",
    "    epoches_done += 1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "58306faf3256e405"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
